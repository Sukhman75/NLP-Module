{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP_Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kid fight\n",
      "['kid']\n",
      "0.0\n",
      "['kid', 'fight']\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "kid\n",
      "fight\n",
      "fight\n",
      "-1\n",
      "Time-Taken: 0.0009970664978027344\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import time\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "\n",
    " \n",
    "start=time.time() \n",
    "#print(data_sample.head(10))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    " \n",
    "def penn_to_wn(tag):\n",
    "    \"\"\"\n",
    "    Convert between the PennTreebank tags to simple Wordnet tags\n",
    "    \"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None\n",
    " \n",
    " \n",
    "def clean_text(text):\n",
    "    text = text.replace(\"<br />\", \" \")\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = \" \".join([lemmatizer.lemmatize(word) for word in tokens if word not in stopwords])\n",
    "    #text = text.decode(\"utf-8\")\n",
    "    return text\n",
    " \n",
    " \n",
    "def swn_polarity(text):\n",
    "    \"\"\"\n",
    "    Return a sentiment polarity: 0 = negative, 1 = positive\n",
    "    \"\"\"\n",
    " \n",
    "    sentiment = 0.0\n",
    "    tokens_count = 0\n",
    " \n",
    "    text = clean_text(text)\n",
    " \n",
    "    print(text)\n",
    "    \n",
    "    tagged_sentence = pos_tag(word_tokenize(text))\n",
    "    #print(tagged_sentence)\n",
    "    wrd_list = []\n",
    "    for word, tag in tagged_sentence:\n",
    "        wn_tag = penn_to_wn(tag)\n",
    "        if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n",
    "            continue\n",
    " \n",
    "        \n",
    "         \n",
    "        #print(word)\n",
    "        wrd_list.append(word)\n",
    "        print(wrd_list)\n",
    "        for x in range(len(wrd_list)):\n",
    "            for wd in wrd_list:\n",
    "                synsets = wn.synsets(wd, pos=wn_tag)\n",
    "                if not synsets:\n",
    "                    continue\n",
    " \n",
    "            # Take the first sense, the most common\n",
    "                synset = synsets[0]\n",
    "                swn_synset = swn.senti_synset(synset.name())\n",
    " \n",
    "                sentiment += swn_synset.pos_score() - swn_synset.neg_score()\n",
    "                tokens_count += 1\n",
    "                #print(tokens_count)\n",
    "                print(sentiment)\n",
    " \n",
    "    # judgment call ? Default to positive or negative\n",
    "    if not tokens_count:\n",
    "        print(\"No tokens\")\n",
    "        #return 0\n",
    "    #neg_words = [\"fight\",\"fighting\",\"kill\", \"killing\",\"death\",\"die\", \"accident\",\"damage\",\"fire\",\"firing\",\"accident\"]\n",
    "    #x_ls = []\n",
    "    f = open(\"negative-words.txt\", \"r\")\n",
    "    words2=f.read()\n",
    "\n",
    "    #text = \"\".join([word.lower() for word in words])\n",
    "    #text = text.replace(\"\\n\", \" \")\n",
    "    #tokens = re.split('\\W+', words)\n",
    "    #x_ls.append(tokens)\n",
    "    \n",
    "    for wrd in wrd_list:\n",
    "        print(wrd)\n",
    "        if wrd in words2:\n",
    "            print(wrd)\n",
    "            return -1\n",
    "    #wrd = \"\".join([word for word in lemma])\n",
    "    #print(wrd)\n",
    "    \n",
    "    #if word in neg_words:\n",
    "        #return -1\n",
    "    #print(word)    \n",
    "    # sum greater than 0 => positive sentiment\n",
    "    if sentiment >= 0:\n",
    "        return 1\n",
    "    \n",
    " \n",
    "    # negative sentiment\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "end=time.time()\n",
    "print(swn_polarity(\"a kid is fight\"))\n",
    "print(\"Time-Taken:\",end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person bleeding\n",
      "bleeding\n",
      "-1\n",
      "Time-Taken: 0.0009982585906982422\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import time\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    " \n",
    "start=time.time() \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    " \n",
    "def clean_text(text):\n",
    "    #text = text.replace(\"\\n\", \" \")\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = \" \".join([lemmatizer.lemmatize(word) for word in tokens if word not in stopwords])\n",
    "    return text\n",
    " \n",
    " \n",
    "def swn_polarity(text):\n",
    "    \"\"\"\n",
    "    Return a sentiment polarity: 0 = negative, 1 = positive\n",
    "    \"\"\"\n",
    " \n",
    "    sentiment = 0.0\n",
    "    tokens_count = 0\n",
    " \n",
    "    text = clean_text(text)\n",
    "    print(text)\n",
    "    \n",
    "    token_word = word_tokenize(text)\n",
    "    \n",
    "    wrd_list = []\n",
    "    for word in token_word : \n",
    "        wrd_list.append(word)\n",
    "        #print(wrd_list)\n",
    "        \n",
    "        for x in range(len(wrd_list)):\n",
    "            for wd in wrd_list:\n",
    "                synsets = wn.synsets(wd)\n",
    "                if not synsets:\n",
    "                    continue\n",
    " \n",
    "            # Take the first sense, the most common\n",
    "                synset = synsets[0]\n",
    "                swn_synset = swn.senti_synset(synset.name())\n",
    "                sentiment += swn_synset.pos_score() - swn_synset.neg_score()\n",
    "                tokens_count += 1\n",
    "                #print(sentiment)\n",
    " \n",
    "    # judgment call ? Default to positive or negative\n",
    "    if not tokens_count:\n",
    "        print(\"No tokens\")\n",
    "       \n",
    "    f = open(\"negative-words.txt\", \"r\")\n",
    "    words2=f.read() \n",
    "    for wrd in wrd_list:\n",
    "        #print(wrd)\n",
    "        if wrd in words2:\n",
    "            print(wrd)\n",
    "            return -1\n",
    "    \n",
    "    # sum greater than 0 => positive sentiment\n",
    "    if sentiment >= 0:\n",
    "        return 1\n",
    "    \n",
    " \n",
    "    # negative sentiment\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "end=time.time()\n",
    "print(swn_polarity(\"A person is and bleeding\"))\n",
    "print(\"Time-Taken:\",end-start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
